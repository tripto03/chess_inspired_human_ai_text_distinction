{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMi4WKgIpesQGPBb48PTzc+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This notebook contains the necessary codes that we utilize to extract various features from the introduction, body, and conclusion of the text.\n","\n","Codes are provided in a Jupyter Notebook for easy interpretation and future modification.\n","\n","We need to start from the data files (reuter/enron/persuade) with introduction, body, conclusion text separated, the corresponding code will add individual features as columns to the dataframes sequentially."],"metadata":{"id":"YuDS7k5oOy-7"}},{"cell_type":"markdown","source":["# Initialization"],"metadata":{"id":"gDiedLusQCA6"}},{"cell_type":"markdown","source":["Install the following library for the feature extractions"],"metadata":{"id":"4vdNcwXLSBwl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-sP8-NhOv2K"},"outputs":[],"source":["!pip install spacy\n","!pip install nltk\n","!pip install textstat\n","!python -m spacy download en_core_web_sm\n","!pip install spacytextblob\n","!python -m textblob.download_corpora\n","!pip install torch\n","!pip install transformers\n","!pip install sentence_transformers\n","!pip install liwc\n","!pip install writeprints-static\n","import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')"]},{"cell_type":"code","source":["from glob import glob\n","import shutil\n","import glob\n","import json\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import defaultdict, Counter\n","from statistics import stdev\n","import networkx as nx\n","import pandas as pd\n","import codecs\n","import requests\n","from bs4 import BeautifulSoup\n","import re\n","import pickle\n","from tqdm import tqdm\n","tqdm.pandas()\n","import random\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import csv\n","from sklearn.metrics import classification_report, precision_score,recall_score\n","from sklearn.model_selection import train_test_split\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","import warnings\n","import email # for handling email data format\n","\n","# Ignore all warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"fz3wWyj-SWds"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature extractions"],"metadata":{"id":"TBoz7Qs4STik"}},{"cell_type":"code","source":["csv_file = 'datasets//reuter.csv'  # you need to replace it with the corresponding dataset csv file\n","df = pd.read_csv(csv_file)"],"metadata":{"id":"MYdkY8tkSp4U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vocabulary richness"],"metadata":{"id":"XcHBIzwySZxU"}},{"cell_type":"markdown","source":["We calculate brunet index as the vocabulary richness metric"],"metadata":{"id":"Xitt4KU4Scp0"}},{"cell_type":"code","source":["def calculate_lexical_div(text):\n","  try:\n","    doc = nlp(text)\n","    tokens = [token.text.lower() for token in doc if not token.is_space and not token.is_punct]\n","    unique_tokens = set(tokens)\n","\n","    vocabulary_size = len(unique_tokens)\n","    total_tokens = len(tokens)\n","    brunets_index = vocabulary_size / (total_tokens ** (0.165))\n","\n","    return  brunets_index\n","  except:\n","    return 0\n"],"metadata":{"id":"qqmpXYNzSb0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["part = 'intro'\n","source_col = part + '_text'\n","feature_names = part + '_brunet_index'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(calculate_lexical_div(s)))\n","\n","part = 'body'\n","source_col = part + '_text'\n","feature_names = part + '_brunet_index'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(calculate_lexical_div(s)))\n","\n","part = 'conclusion'\n","source_col = part + '_text'\n","feature_names = part + '_brunet_index'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(calculate_lexical_div(s)))"],"metadata":{"id":"6Xc9p6rDS74s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## redability score"],"metadata":{"id":"2HAXbYf3TIAc"}},{"cell_type":"code","source":["import textstat\n","\n","def calculate_readability_scores(text):\n","    try:\n","      # Flesch Reading Ease Score\n","      flesch_reading_ease = textstat.flesch_reading_ease(text)\n","      return flesch_reading_ease\n","    except:\n","      return 0"],"metadata":{"id":"_FtiTvurTKQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["part = 'intro'\n","source_col = part + '_text'\n","feature_names = part + '_readability_score'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(calculate_readability_scores(s)))\n","\n","part = 'body'\n","source_col = part + '_text'\n","feature_names = part + '_readability_score'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(calculate_readability_scores(s)))\n","\n","part = 'conclusion'\n","source_col = part + '_text'\n","feature_names = part + '_readability_score'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(calculate_readability_scores(s)))"],"metadata":{"id":"diSAOFWYTWeN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pos-tags distributions"],"metadata":{"id":"JB4cA7lvTkAN"}},{"cell_type":"code","source":["import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","def get_pos_tag_counts(text):\n","    # Process the text using spaCy\n","    pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n","     # Initialize a Counter with all possible POS tags\n","    pos_tag_counts = Counter({tag: 0 for tag in pos_tags})\n","    try:\n","      doc = nlp(text)\n","      # Iterate through each token in the document and update the counts of POS tags\n","      for token in doc:\n","          pos_tag_counts[token.pos_] += 1\n","\n","      # Convert Counter to dictionary\n","      pos_tag_map = dict(pos_tag_counts)\n","\n","      return pos_tag_map\n","    except:\n","      return dict(pos_tag_counts)"],"metadata":{"id":"y2Jp2ctWTlLG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["part = 'intro'\n","source_col = part + '_text'\n","feature_names = part + '_pos_tag_counts'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_pos_tag_counts(s)))\n","\n","part = 'body'\n","source_col = part + '_text'\n","feature_names = part + '_pos_tag_counts'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_pos_tag_counts(s)))\n","\n","part = 'conclusion'\n","source_col = part + '_text'\n","feature_names = part + '_pos_tag_counts'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_pos_tag_counts(s)))"],"metadata":{"id":"UBJ7AcyfTszF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## named-entity distributions"],"metadata":{"id":"H1qGJAcCT7f9"}},{"cell_type":"code","source":["def get_named_entity_counts(text):\n","\n","    # Initialize a Counter with all possible NER tags\n","    ner_tags = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n","    ner_counts = Counter({tag: 0 for tag in ner_tags})\n","\n","    try:\n","      # Process the text using spaCy\n","      doc = nlp(text)\n","      # Iterate through each entity in the document and update the counts of NER tags\n","      for ent in doc.ents:\n","          ner_counts[ent.label_] += 1\n","\n","      # Convert Counter to dictionary\n","      ner_tag_map = dict(ner_counts)\n","\n","      return ner_tag_map\n","    except:\n","      return dict(ner_counts)"],"metadata":{"id":"zeKSCFzLT-EW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["part = 'intro'\n","source_col = part + '_text'\n","feature_names = part + '_ner_counts'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_named_entity_counts(s)))\n","\n","part = 'body'\n","source_col = part + '_text'\n","feature_names = part + '_ner_counts'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_named_entity_counts(s)))\n","\n","part = 'conclusion'\n","source_col = part + '_text'\n","feature_names = part + '_ner_counts'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_named_entity_counts(s)))"],"metadata":{"id":"2znt0bEEUBrG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stopwords distributions"],"metadata":{"id":"EKTuivbqUIPm"}},{"cell_type":"code","source":["def get_stopwords_count(text):\n","  try:\n","    # Process the text with SpaCy\n","    text = text.lower()\n","    # print(text)\n","    doc = nlp(text)\n","\n","    # Initialize a Counter to count stopwords\n","    stopwords_count = Counter()\n","\n","    # Iterate over tokens in the document\n","    for token in doc:\n","        # Check if the token is a stopword\n","        if token.is_stop:\n","            # Increment the count of the stopword\n","            stopwords_count[token.text] += 1\n","    stopwords_count_desc = dict(sorted(stopwords_count.items(), key=lambda item: item[1], reverse=True))\n","    return dict(stopwords_count_desc)\n","  except:\n","    return {}"],"metadata":{"id":"Q4j0JoXDVsQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["part = 'intro'\n","source_col = part + '_text'\n","feature_names = part + '_stopwords_count'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_stopwords_count(s)))\n","\n","part = 'body'\n","source_col = part + '_text'\n","feature_names = part + '_stopwords_count'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_stopwords_count(s)))\n","\n","part = 'conclusion'\n","source_col = part + '_text'\n","feature_names = part + '_stopwords_count'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_stopwords_count(s)))"],"metadata":{"id":"t_BAjJ7oVvDP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## sentiment analysis"],"metadata":{"id":"_mb6GAdGV5L3"}},{"cell_type":"markdown","source":["For sentiment analysis, we divide the text part into sentences. For each sentence, we calculate the polarity score, subjectivity score, and count of positive and negative words in that sentence. While evaluating the whole text part, we averaged over the sentences."],"metadata":{"id":"1CGlRrGrWKC3"}},{"cell_type":"code","source":["def get_sentiment_scores(text):\n","  try:\n","      # Tokenize the text into sentences\n","      sentences = nltk.sent_tokenize(text)\n","\n","      # Compute embeddings for each sentence\n","      pos_wc = 0\n","      neg_wc = 0\n","      polarity_scores = []\n","      subjectivity_scores = []\n","      for sentence in sentences:\n","          doc = nlp(sentence)\n","          polarity_scores.append(doc._.blob.polarity)\n","          subjectivity_scores.append(doc._.blob.subjectivity)\n","          for x in doc._.blob.sentiment_assessments.assessments:\n","            if x[1] > 0:\n","              pos_wc = pos_wc + len(x[0])\n","            elif x[1] < 0:\n","              neg_wc = neg_wc + len(x[0])\n","            else:\n","              pass\n","      return polarity_scores, subjectivity_scores, pos_wc, neg_wc\n","  except:\n","      return [],[],0,0"],"metadata":{"id":"kDEuHVUmV7cn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["part = 'intro'\n","source_col = part + '_text'\n","F = ['polarity_scores','subjectivity_scores','pos_wc','neg_wc',]\n","feature_names = [part+'_'+f for f in F]\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_sentiment_scores(s)))\n","\n","part = 'body'\n","source_col = part + '_text'\n","F = ['polarity_scores','subjectivity_scores','pos_wc','neg_wc',]\n","feature_names = [part+'_'+f for f in F]\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_sentiment_scores(s)))\n","\n","part = 'conclusion'\n","source_col = part + '_text'\n","F = ['polarity_scores','subjectivity_scores','pos_wc','neg_wc',]\n","feature_names = [part+'_'+f for f in F]\n","df[feature_names] = = df[source_col].progress_apply(lambda s: pd.Series(get_sentiment_scores(s)))"],"metadata":{"id":"CVoEAWS3Wrfo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Formality score"],"metadata":{"id":"0VVFqtM5XA14"}},{"cell_type":"markdown","source":["Similar to sentiment analysis, we calculate the formality score for each sentence and then average over the sentences to get the formality score for the whole text part. We utilize https://huggingface.co/s-nlp/roberta-base-formality-ranker from HuggingFace for formality calculation."],"metadata":{"id":"b49nB_63XDf4"}},{"cell_type":"code","source":["# Use a pipeline as a high-level helper\n","from transformers import pipeline\n","pipe = pipeline(\"text-classification\", model=\"s-nlp/roberta-base-formality-ranker\")\n","\n","def get_formality_scores(text):\n","  try:\n","      # Tokenize the text into sentences\n","      sentences = nltk.sent_tokenize(text)\n","\n","      # Compute formality for each sentence\n","      scores = []\n","      for sentence in sentences:\n","          result = pipe(sentence)\n","          if result[0]['label'] == 'informal':\n","            score = round(1 - result[0]['score'],2)\n","          else:\n","            score = round(result[0]['score'],2)\n","          scores.append(score)\n","      return scores\n","  except:\n","      return []"],"metadata":{"id":"fYd18niqXWC5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['intro_formality_score'] = df['intro_text'].progress_apply(get_formality_scores)\n","df['body_formality_score'] = df['body_text'].progress_apply(get_formality_scores)\n","df['conclusion_formality_score'] = df['conclusion_text'].progress_apply(get_formality_scores)"],"metadata":{"id":"IJZ0M0brXqTZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Text embedding (for content similarity and change)"],"metadata":{"id":"H12HaQflYGpP"}},{"cell_type":"markdown","source":[" we get the total text embedding using the OpenAI embedding, which has a higher context length."],"metadata":{"id":"1jg63N2wYMDh"}},{"cell_type":"code","source":["import openai\n","\n","openai.api_key = \"YOUR_API_KEY\"\n","\n","def get_embedding(text, model=\"text-embedding-ada-002\"):\n","    response = openai.Embedding.create(\n","        input=text,\n","        model=model\n","    )\n","    return response['data'][0]['embedding']"],"metadata":{"id":"apgdMLkPY5Ip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["part = 'intro'\n","source_col = part + '_text'\n","feature_names = part + '_embedding'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_embedding(s)))\n","\n","part = 'body'\n","source_col = part + '_text'\n","feature_names = part + '_embedding'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_embedding(s)))\n","\n","part = 'conclusion'\n","source_col = part + '_text'\n","feature_names = part + '_embedding'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_embedding(s)))"],"metadata":{"id":"TMkFAL6TY7RS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Perplexity scores calculations"],"metadata":{"id":"8Z0f-WuEZMFz"}},{"cell_type":"markdown","source":["We calculate the perplexity of total text using a small language model (ex: GPT-2-xl). The output will be of the size [token_length] of the text parts that we are considering."],"metadata":{"id":"rJRS-FQvZeQT"}},{"cell_type":"code","source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","# Load pre-trained GPT2 model and tokenizer\n","model_name = \"gpt2-xl\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","def get_perplexity_score(text):\n","  try:\n","    # Tokenize the text\n","    tokenized_text = tokenizer.encode(text, return_tensors=\"pt\")\n","    # Keep only the first 1024 tokens if the length exceeds the limit\n","    if tokenized_text.size(1) > 1024:\n","        tokenized_text = tokenized_text[:, :1024]\n","    # Calculate conditional probabilities for each token\n","    with torch.no_grad():\n","        outputs = model(tokenized_text)\n","        logits = outputs.logits[0]  # Logits for the last layer\n","        softmax_scores = torch.softmax(logits, dim=-1)\n","\n","    prob_scores = softmax_scores.tolist()\n","    perplexity_scores = []\n","    for i,token in enumerate(tokenized_text[0]):\n","      # print(\"Token: \",tokenizer.decode([token.item()]),token.item(),\" surprisal: \",-np.log(prob_scores[i][token.item()]))\n","      perplexity_scores.append(-np.log(prob_scores[i][token.item()]))\n","    return perplexity_scores\n","  except:\n","    return []"],"metadata":{"id":"hldcT0VfZeBb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["part = 'intro'\n","source_col = part + '_text'\n","feature_names = part + '_perplexity_score'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_perplexity_score(s)))\n","\n","part = 'body'\n","source_col = part + '_text'\n","feature_names = part + '_perplexity_score'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_perplexity_score(s)))\n","\n","part = 'conclusion'\n","source_col = part + '_text'\n","feature_names = part + '_perplexity_score'\n","df[feature_names] = df[source_col].progress_apply(lambda s: pd.Series(get_perplexity_score(s)))"],"metadata":{"id":"qnf1JuNrZOtj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## avg perplexity and burstiness"],"metadata":{"id":"fH8TSi0vZ4tC"}},{"cell_type":"markdown","source":["We calculate the average perplexity and burstiness of the text part using the GPTZERO API directly."],"metadata":{"id":"4Q1DgwKlZ7iD"}},{"cell_type":"code","source":["import time\n","import pprint\n","import requests\n","url = \"https://api.gptzero.me/v2/predict/text\"\n","gptzero_api_key = \"YOUR_API_KEY\"\n","headers = {\n","    \"Accept\": \"application/json\",\n","    \"Content-Type\": \"application/json\",\n","    \"x-api-key\": gptzero_api_key}\n","def get_preplexity_burstiness(text):\n","  try:\n","    data = {\n","      \"document\": text,\n","      \"version\": \"2024-01-09\",\n","      \"multilingual\": False,\n","      \"writing_stats\": True\n","    }\n","    try_counter = 0\n","    while try_counter <2:\n","      try:\n","          response = requests.post(url, headers=headers, json=data,).json()\n","          time.sleep(0.5)\n","          break\n","      except:\n","          try_counter += 1\n","          if try_counter == 1:\n","              return [], 'NA', 'NA', 'NA'\n","          continue\n","    perplexities = []\n","    for k in response['documents'][0]['sentences']:\n","      perplexities.append(round(k['perplexity'],2))\n","    avg_perplexity = round(np.average(perplexities),2)\n","    std_perplexity = round(np.std(perplexities),2)\n","    avg_burstiness = round(response['documents'][0]['overall_burstiness'],2)\n","    return perplexities, avg_perplexity,std_perplexity, avg_burstiness\n","  except:\n","    return [], 'NA', 'NA', 'NA'\n"],"metadata":{"id":"FiUHpLpBaEaj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We iterately run over in our case to store the values frequently since each iteration is costly"],"metadata":{"id":"8PmmgCO2ahsk"}},{"cell_type":"code","source":["def get_preplexity_burstiness(part):\n","  # Define the iteration interval for saving the DataFrame\n","  save_interval = 100  # Save after every 100 iterations\n","  col = part + '_text'\n","  perplexity_col =part +  '_perplexity_scores'\n","  avg_perplexity_col =part +  '_avg_perplexity_score'\n","  std_perplexity_col =part +  '_std_perplexity_score'\n","  avg_burstiness_col =part +  '_avg_burstiness_score'\n","  df[perplexity_col] = ''\n","  df[avg_perplexity_col] = ''\n","  df[std_perplexity_col] = ''\n","  df[avg_burstiness_col] = ''\n","  # Apply the function to each row and insert the result into a new column\n","  for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n","      # Process the row\n","      scores = get_preplexity_burstiness(row[col])\n","\n","      # Insert the processed value into a new column\n","      df.at[i, perplexity_col] = scores[0]\n","      df.at[i, avg_perplexity_col] = scores[1]\n","      df.at[i, std_perplexity_col] = scores[2]\n","      df.at[i, avg_burstiness_col] = scores[3]\n","\n","      # Check if it's time to save the DataFrame\n","      if (i+1) % save_interval == 0:\n","          print(scores)\n","          df.to_csv(csv_file, index=False)  # Save DataFrame to CSV file\n","          print(f\"Saved DataFrame after {i+1} iterations.\")\n","\n","  # Save the final DataFrame\n","  df.to_csv(csv_file, index=False)"],"metadata":{"id":"oYe8M9Epar-8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["part = 'intro'\n","get_preplexity_burstiness(part)\n","\n","part = 'body'\n","get_preplexity_burstiness(part)\n","\n","part = 'conclusion'\n","get_preplexity_burstiness(part)"],"metadata":{"id":"Bbq1Qq-JaYr7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LIWC features"],"metadata":{"id":"DCpwkl7ObAS8"}},{"cell_type":"code","source":["parse, category_names = liwc.load_token_parser('REPLACE_WITH_CORRESPONDING_LIWC_DICTIONARY')\n","def tokenize(text):\n","    tokens = nltk.word_tokenize(text)\n","    return tokens\n","def get_liwc_features(text):\n","  text = text.lower()\n","  tokenlist = tokenize(text)\n","  STYLE_FEATURES = {}\n","  token_counts = Counter(category for token in tokenlist for category in parse(token))\n","  total_words = len(tokenlist)\n","  for category in category_names:\n","    f = category + '_frac'\n","    if category not in token_counts.keys():  # then frequency 0\n","      STYLE_FEATURES[f] = 0\n","    else:\n","      STYLE_FEATURES[f] = token_counts[category]\n","  return  STYLE_FEATURES"],"metadata":{"id":"JznSv_qLbCkk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["part = 'intro'\n","source_col = part + '_text'\n","feature_names = part + '_liwc_features'\n","df[feature_names] = df[source_col].progress_apply(get_liwc_features)\n","\n","part = 'body'\n","source_col = part + '_text'\n","feature_names = part + '_liwc_features'\n","df[feature_names] = df[source_col].progress_apply(get_liwc_features)\n","\n","part = 'conclusion'\n","source_col = part + '_text'\n","feature_names = part + '_liwc_features'\n","df[feature_names] = df[source_col].progress_apply(get_liwc_features)"],"metadata":{"id":"6GgOxFVkcRw1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Writeprint features"],"metadata":{"id":"jWTb3-X7chHd"}},{"cell_type":"code","source":["from writeprints_static import WriteprintsStatic\n","vec = WriteprintsStatic()\n","texts = [\"a sample text\",]\n","X = vec.transform(texts)\n","features = list(vec.get_feature_names()) # extract the feature names on a sample text so that we can use them to populate the dictionary with % value\n","\n","def count_char_bigrams(text):\n","    char_bigrams = [text[i:i+2] for i in range(len(text)-1) if text[i] != ' ' and text[i+1] != ' ']\n","    return len(char_bigrams)\n","\n","def count_char_trigrams(text):\n","    char_trigrams = [text[i:i+3] for i in range(len(text)-2) if text[i] != ' ' and text[i+1] != ' ' and text[i+2] != ' ']\n","    return len(char_trigrams)\n","def extract_from_writeprint_features(text):\n","  STYLE_FEATURES = {}\n","  X = vec.transform([text])\n","\n","  # to check the feature values\n","  X = X.toarray()[0]\n","  feature_names = list(vec.get_feature_names())\n","  W = X[features.index('total_words')]\n","  C = X[features.index('total_chars')]\n","  B = count_char_bigrams(text)\n","  T = count_char_trigrams(text)\n","  # print(W,C,B,T)\n","\n","  # letter features\n","  letter_features = [string for string in feature_names if string.startswith('letter')]\n","  for f in letter_features:\n","    STYLE_FEATURES[f] = X[features.index(f)] / C\n","  # print(STYLE_FEATURES)\n","\n","  # digit features\n","  letter_features = [string for string in feature_names if string.startswith('digit_')]\n","  for f in letter_features:\n","    STYLE_FEATURES[f] = X[features.index(f)] / C\n","\n","  # special chars features\n","  letter_features = [string for string in feature_names if string.startswith('special_char_')]\n","  for f in letter_features:\n","    STYLE_FEATURES[f] = X[features.index(f)] / C\n","\n","  # bigram features\n","  letter_features = [string for string in feature_names if string.startswith('bigram_')]\n","  for f in letter_features:\n","    STYLE_FEATURES[f] = X[features.index(f)] / B\n","\n","\n","  # trigram features\n","  letter_features = [string for string in feature_names if string.startswith('trigram_')]\n","  for f in letter_features:\n","    STYLE_FEATURES[f] = X[features.index(f)] / T\n","\n","  # function word features\n","  letter_features = [string for string in feature_names if string.startswith('function_')]\n","  for f in letter_features:\n","    STYLE_FEATURES[f] = X[features.index(f)] / W\n","\n","  # function word features\n","  letter_features = [string for string in feature_names if string.startswith('pos_')]\n","  for f in letter_features:\n","    STYLE_FEATURES[f] = X[features.index(f)] / W\n","\n","  STYLE_FEATURES['hapax_legomena_ratio'] = X[features.index('hapax_legomena_ratio')]\n","  STYLE_FEATURES['dis_legomena_ratio'] = X[features.index('dis_legomena_ratio')]\n","  STYLE_FEATURES['avg_word_length'] = X[features.index('avg_word_length')]\n","  STYLE_FEATURES['short_words'] = X[features.index('short_words')]/W\n","  STYLE_FEATURES['digits_ratio'] = X[features.index('digits_ratio')]\n","\n","  return STYLE_FEATURES"],"metadata":{"id":"qXPFvPBKcj5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['intro_writeprint_features'] = df['intro_text'].progress_apply(extract_from_writeprint_features)\n","df['body_writeprint_features'] = df['body_text'].progress_apply(extract_from_writeprint_features)\n","df['conclusion_writeprint_features'] = df['conclusion_text'].progress_apply(extract_from_writeprint_features)"],"metadata":{"id":"3DnV3503dOoO"},"execution_count":null,"outputs":[]}]}